{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 통합 테스트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection > Tranlation > Text to Voice / 10초 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "model\\ram_plus_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from model\\ram_plus_swin_large_14m.pth\n",
      "vit: swin_l\n",
      "Most Confident Object: apple \n",
      "Korean: 사과, Chinese: 苹果, Japanese: りんご\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI_1 Object Detection\n",
    "# STEP 1 : import modules\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import inference_ram as inference\n",
    "from ram import get_transform\n",
    "\n",
    "# STEP 2: create inference object\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"model\\\\ram_plus_swin_large_14m.pth\"\n",
    "model = ram_plus(pretrained=model_path,\n",
    "                            image_size=384,\n",
    "                            vit='swin_l')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# STEP 3: Load data\n",
    "image_path = \"image\\\\1641173_2291260800.jpg\"\n",
    "transform = get_transform(image_size=384)\n",
    "image = transform(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# STEP 4: inference\n",
    "res = inference(image, model)\n",
    "result = []\n",
    "# STEP 5: post processing\n",
    "if res and len(res[0]) > 0:\n",
    "    result = res[0].split(\"|\")\n",
    "    print(\"Most Confident Object:\", result[0])\n",
    "else:\n",
    "    print(\"No objects detected.\")\n",
    "\n",
    "# AI2 Translations\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "word = result[0]\n",
    "korean = translator.translate(word, src=\"en\", dest=\"ko\").text\n",
    "chinese = translator.translate(word, src=\"en\", dest=\"zh-cn\").text\n",
    "japanese = translator.translate(word, src=\"en\", dest=\"ja\").text\n",
    "\n",
    "print(f\"Korean: {korean}, Chinese: {chinese}, Japanese: {japanese}\")\n",
    "\n",
    "# AI3 Text to KorSpeech\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "en_text = word\n",
    "kr_text = korean\n",
    "ch_text = chinese\n",
    "jp_text = japanese\n",
    "\n",
    "en_tts = gTTS(text = en_text, lang='en')\n",
    "kr_tts = gTTS(text = kr_text, lang='ko')\n",
    "ch_tts = gTTS(text = ch_text, lang='zh-cn')\n",
    "jp_tts = gTTS(text = jp_text, lang='ja')\n",
    "\n",
    "en_tts.save(\"audio\\\\en_output.mp3\")\n",
    "kr_tts.save(\"audio\\\\kr_output.mp3\")\n",
    "ch_tts.save(\"audio\\\\ch_output.mp3\")\n",
    "jp_tts.save(\"audio\\\\jp_output.mp3\")\n",
    "\n",
    "os.system(\"start audio\\\\en_output.mp3\")\n",
    "# os.system(\"start output2.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "model\\ram_plus_swin_large_14m.pth\n",
      "--------------\n",
      "load checkpoint from model\\ram_plus_swin_large_14m.pth\n",
      "vit: swin_l\n",
      "Most Confident Object: apple \n"
     ]
    }
   ],
   "source": [
    "# STEP 1 : import modules\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import inference_ram as inference\n",
    "from ram import get_transform\n",
    "\n",
    "# STEP 2: create inference object\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"model\\\\ram_plus_swin_large_14m.pth\"\n",
    "model = ram_plus(pretrained=model_path,\n",
    "                            image_size=384,\n",
    "                            vit='swin_l')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# STEP 3: Load data\n",
    "image_path = \"image\\\\1641173_2291260800.jpg\"\n",
    "transform = get_transform(image_size=384)\n",
    "image = transform(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# STEP 4: inference\n",
    "res = inference(image, model)\n",
    "\n",
    "# STEP 5: post processing\n",
    "if res and len(res[0]) > 0:\n",
    "    result = res[0].split(\"|\")\n",
    "    print(\"Most Confident Object:\", result[0])\n",
    "else:\n",
    "    print(\"No objects detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 사과, Chinese: 苹果, Japanese: りんご\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "word = \"apple\"\n",
    "korean = translator.translate(word, src=\"en\", dest=\"ko\").text\n",
    "chinese = translator.translate(word, src=\"en\", dest=\"zh-cn\").text\n",
    "japanese = translator.translate(word, src=\"en\", dest=\"ja\").text\n",
    "\n",
    "print(f\"Korean: {korean}, Chinese: {chinese}, Japanese: {japanese}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "text1 = \"hello, i'm test program, 안녕 나는 테스트 프로그램이야\"\n",
    "\n",
    "tts1 = gTTS(text = text1, lang='en')\n",
    "# tts2 = gTTS(text = text, lang='ko')\n",
    "\n",
    "tts1.save(\"output1.mp3\")\n",
    "# tts2.save(\"output2.mp3\")\n",
    "\n",
    "os.system(\"start output1.mp3\")\n",
    "# os.system(\"start output2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 음성의 유클리드 거리: 0.0\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# 음성 파일 로드\n",
    "def load_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    return y, sr\n",
    "\n",
    "# 멜 스펙트로그램 생성\n",
    "def compute_mel_spectrogram(y, sr):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    return mel_spectrogram\n",
    "\n",
    "# 유클리드 거리 계산\n",
    "def euclidean_distance(spectrogram1, spectrogram2):\n",
    "    # 스펙트로그램을 1차원으로 변환 후 유클리드 거리 계산\n",
    "    dist = np.linalg.norm(spectrogram1 - spectrogram2)\n",
    "    return dist\n",
    "\n",
    "# 파일 경로 설정\n",
    "file1 = 'output.mp3'  # 첫 번째 음성 파일\n",
    "file2 = 'output1.mp3'  # 두 번째 음성 파일\n",
    "\n",
    "# 음성 파일 로드\n",
    "y1, sr1 = load_audio(file1)\n",
    "y2, sr2 = load_audio(file2)\n",
    "\n",
    "# 멜 스펙트로그램 생성\n",
    "mel_spectrogram1 = compute_mel_spectrogram(y1, sr1)\n",
    "mel_spectrogram2 = compute_mel_spectrogram(y2, sr2)\n",
    "\n",
    "# 유클리드 거리 계산\n",
    "distance = euclidean_distance(mel_spectrogram1, mel_spectrogram2)\n",
    "\n",
    "print(f'두 음성의 유클리드 거리: {distance}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
